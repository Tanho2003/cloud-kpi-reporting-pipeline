{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: c:\\Users\\TAN\\OneDrive\\Cloud_KPI_Reporting_Pipeline\\notebooks\n",
      "ROOT: c:\\Users\\TAN\\OneDrive\\Cloud_KPI_Reporting_Pipeline\n",
      "RAW_DIR: c:\\Users\\TAN\\OneDrive\\Cloud_KPI_Reporting_Pipeline\\data\\raw\\olist\\2026-01-12\n",
      "Orders exists?: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "# Find the project root by walking upward until we see the /data folder\n",
    "ROOT = None\n",
    "for p in [Path.cwd(), *Path.cwd().parents]:\n",
    "    if (p / \"data\").exists() and (p / \"reports\").exists():\n",
    "        ROOT = p\n",
    "        break\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "\n",
    "RAW_DIR = ROOT / \"data/raw/olist/2026-01-12\"\n",
    "OUT_DIR = ROOT / \"data/curated/olist/2026-01-12\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"RAW_DIR:\", RAW_DIR)\n",
    "print(\"Orders exists?:\", (RAW_DIR / \"olist_orders_dataset.csv\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders (99441, 8)\n",
      "items (112650, 7)\n",
      "payments (103886, 5)\n",
      "customers (99441, 5)\n",
      "products (32951, 9)\n",
      "trans (71, 2)\n",
      "\n",
      "order_status counts:\n",
      " order_status\n",
      "delivered      96478\n",
      "shipped         1107\n",
      "canceled         625\n",
      "unavailable      609\n",
      "invoiced         314\n",
      "processing       301\n",
      "created            5\n",
      "approved           2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "orders = pd.read_csv(\n",
    "    RAW_DIR/\"olist_orders_dataset.csv\",\n",
    "    parse_dates=[\n",
    "        \"order_purchase_timestamp\",\"order_approved_at\",\n",
    "        \"order_delivered_carrier_date\",\"order_delivered_customer_date\",\n",
    "        \"order_estimated_delivery_date\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "items = pd.read_csv(RAW_DIR/\"olist_order_items_dataset.csv\", parse_dates=[\"shipping_limit_date\"])\n",
    "payments = pd.read_csv(RAW_DIR/\"olist_order_payments_dataset.csv\")\n",
    "customers = pd.read_csv(RAW_DIR/\"olist_customers_dataset.csv\")\n",
    "products = pd.read_csv(RAW_DIR/\"olist_products_dataset.csv\")\n",
    "trans = pd.read_csv(RAW_DIR/\"product_category_name_translation.csv\")\n",
    "\n",
    "print(\"orders\", orders.shape)\n",
    "print(\"items\", items.shape)\n",
    "print(\"payments\", payments.shape)\n",
    "print(\"customers\", customers.shape)\n",
    "print(\"products\", products.shape)\n",
    "print(\"trans\", trans.shape)\n",
    "print(\"\\norder_status counts:\\n\", orders[\"order_status\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_date saved: (800, 11)\n",
      "min/max: 2016-09-04 → 2018-11-12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# date range: cover purchase + delivered + estimated dates\n",
    "min_date = orders[\"order_purchase_timestamp\"].dt.date.min()\n",
    "\n",
    "max_date = pd.concat([\n",
    "    orders[\"order_purchase_timestamp\"].dropna(),\n",
    "    orders[\"order_delivered_customer_date\"].dropna(),\n",
    "    orders[\"order_estimated_delivery_date\"].dropna()\n",
    "]).dt.date.max()\n",
    "\n",
    "date_range = pd.date_range(min_date, max_date, freq=\"D\")\n",
    "\n",
    "dim_date = pd.DataFrame({\"date\": date_range})\n",
    "dim_date[\"date_id\"] = dim_date[\"date\"].dt.strftime(\"%Y%m%d\").astype(int)\n",
    "dim_date[\"year\"] = dim_date[\"date\"].dt.year\n",
    "dim_date[\"quarter\"] = dim_date[\"date\"].dt.quarter\n",
    "dim_date[\"month_num\"] = dim_date[\"date\"].dt.month\n",
    "dim_date[\"month_name\"] = dim_date[\"date\"].dt.strftime(\"%B\")\n",
    "dim_date[\"year_month\"] = dim_date[\"date\"].dt.strftime(\"%Y-%m\")\n",
    "dim_date[\"week_num\"] = dim_date[\"date\"].dt.isocalendar().week.astype(int)\n",
    "dim_date[\"day_of_week_num\"] = dim_date[\"date\"].dt.dayofweek + 1  # Mon=1\n",
    "dim_date[\"day_of_week_name\"] = dim_date[\"date\"].dt.strftime(\"%A\")\n",
    "dim_date[\"is_weekend\"] = dim_date[\"day_of_week_num\"].isin([6, 7]).astype(int)\n",
    "\n",
    "dim_date = dim_date[[\n",
    "    \"date_id\",\"date\",\"year\",\"quarter\",\"month_num\",\"month_name\",\"year_month\",\n",
    "    \"week_num\",\"day_of_week_num\",\"day_of_week_name\",\"is_weekend\"\n",
    "]]\n",
    "\n",
    "dim_date.to_csv(OUT_DIR/\"dim_date.csv\", index=False)\n",
    "\n",
    "print(\"dim_date saved:\", dim_date.shape)\n",
    "print(\"min/max:\", dim_date[\"date\"].min().date(), \"→\", dim_date[\"date\"].max().date())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_product saved: (32951, 7)\n",
      "EN category coverage: 98.11 %\n",
      "missing EN categories: 623\n"
     ]
    }
   ],
   "source": [
    "# Build dim_product by joining products to translation table\n",
    "dim_product = products.merge(trans, on=\"product_category_name\", how=\"left\")\n",
    "\n",
    "# Rename for curated naming\n",
    "dim_product = dim_product.rename(columns={\n",
    "    \"product_category_name_english\": \"product_category_name_en\"\n",
    "})\n",
    "\n",
    "# Keep only curated columns\n",
    "dim_product = dim_product[[\n",
    "    \"product_id\",\n",
    "    \"product_category_name\",\n",
    "    \"product_category_name_en\",\n",
    "    \"product_weight_g\",\n",
    "    \"product_length_cm\",\n",
    "    \"product_height_cm\",\n",
    "    \"product_width_cm\"\n",
    "]]\n",
    "\n",
    "dim_product.to_csv(OUT_DIR/\"dim_product.csv\", index=False)\n",
    "\n",
    "coverage = dim_product[\"product_category_name_en\"].notna().mean()\n",
    "\n",
    "print(\"dim_product saved:\", dim_product.shape)\n",
    "print(\"EN category coverage:\", round(coverage*100, 2), \"%\")\n",
    "print(\"missing EN categories:\", dim_product[\"product_category_name_en\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_customer saved: (96096, 5)\n",
      "unique customer_unique_id: 96096\n",
      "null customer_state: 0\n"
     ]
    }
   ],
   "source": [
    "# Map orders -> customers so we can pick one row per customer_unique_id\n",
    "cust_orders = orders[[\"customer_id\", \"order_purchase_timestamp\"]].merge(\n",
    "    customers, on=\"customer_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Latest purchase rule: sort ascending and take the last row per customer_unique_id\n",
    "cust_orders = cust_orders.sort_values(\"order_purchase_timestamp\")\n",
    "dim_customer = cust_orders.groupby(\"customer_unique_id\", as_index=False).tail(1)\n",
    "\n",
    "dim_customer = dim_customer[[\n",
    "    \"customer_unique_id\",\n",
    "    \"customer_id\",\n",
    "    \"customer_zip_code_prefix\",\n",
    "    \"customer_city\",\n",
    "    \"customer_state\"\n",
    "]]\n",
    "\n",
    "dim_customer.to_csv(OUT_DIR/\"dim_customer.csv\", index=False)\n",
    "\n",
    "print(\"dim_customer saved:\", dim_customer.shape)\n",
    "print(\"unique customer_unique_id:\", dim_customer[\"customer_unique_id\"].nunique())\n",
    "print(\"null customer_state:\", dim_customer[\"customer_state\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_orders saved: (99441, 16)\n",
      "patched delivered orders (missing payments): 1\n",
      "delivered orders still missing payment_total: 0\n",
      "null customer_unique_id: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Aggregate payments to order_id\n",
    "pay_agg = payments.groupby(\"order_id\", as_index=False).agg(\n",
    "    payment_total=(\"payment_value\", \"sum\"),\n",
    "    payment_count=(\"payment_value\", \"size\")\n",
    ")\n",
    "\n",
    "# 2) Build base fact from orders + customer_unique_id\n",
    "fact_orders = orders.merge(\n",
    "    customers[[\"customer_id\", \"customer_unique_id\"]],\n",
    "    on=\"customer_id\",\n",
    "    how=\"left\"\n",
    ").merge(\n",
    "    pay_agg,\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 3) Patch edge case: delivered orders missing payment_total -> fill with sum(price+freight)\n",
    "items_tmp = items.copy()\n",
    "items_tmp[\"item_value\"] = items_tmp[\"price\"] + items_tmp[\"freight_value\"]\n",
    "item_sum = items_tmp.groupby(\"order_id\", as_index=False).agg(item_value_sum=(\"item_value\", \"sum\"))\n",
    "\n",
    "mask_missing_pay = (fact_orders[\"order_status\"] == \"delivered\") & (fact_orders[\"payment_total\"].isna())\n",
    "\n",
    "fact_orders = fact_orders.merge(item_sum, on=\"order_id\", how=\"left\")\n",
    "fact_orders.loc[mask_missing_pay, \"payment_total\"] = fact_orders.loc[mask_missing_pay, \"item_value_sum\"]\n",
    "fact_orders.loc[mask_missing_pay, \"payment_count\"] = 0  # 0 = patched/unknown payment rows\n",
    "fact_orders = fact_orders.drop(columns=[\"item_value_sum\"])\n",
    "\n",
    "# 4) Date IDs\n",
    "fact_orders[\"purchase_date_id\"] = fact_orders[\"order_purchase_timestamp\"].dt.strftime(\"%Y%m%d\").astype(int)\n",
    "\n",
    "# delivered_date_id as nullable integer (Int64)\n",
    "fact_orders[\"delivered_date_id\"] = fact_orders[\"order_delivered_customer_date\"].dt.strftime(\"%Y%m%d\")\n",
    "fact_orders.loc[fact_orders[\"order_delivered_customer_date\"].isna(), \"delivered_date_id\"] = pd.NA\n",
    "fact_orders[\"delivered_date_id\"] = fact_orders[\"delivered_date_id\"].astype(\"Int64\")\n",
    "\n",
    "\n",
    "# 5) Flags + delivery metrics\n",
    "fact_orders[\"is_delivered\"] = (fact_orders[\"order_status\"] == \"delivered\").astype(int)\n",
    "fact_orders[\"is_canceled_or_unavailable\"] = fact_orders[\"order_status\"].isin([\"canceled\", \"unavailable\"]).astype(int)\n",
    "\n",
    "fact_orders[\"delivery_days\"] = (\n",
    "    (fact_orders[\"order_delivered_customer_date\"] - fact_orders[\"order_purchase_timestamp\"])\n",
    "    .dt.total_seconds() / 86400\n",
    ")\n",
    "\n",
    "fact_orders[\"on_time_flag\"] = np.where(\n",
    "    (fact_orders[\"order_delivered_customer_date\"].notna()) & (fact_orders[\"order_estimated_delivery_date\"].notna()),\n",
    "    (fact_orders[\"order_delivered_customer_date\"] <= fact_orders[\"order_estimated_delivery_date\"]).astype(int),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# 6) Select curated columns + export\n",
    "fact_orders = fact_orders[[\n",
    "    \"order_id\", \"customer_unique_id\", \"order_status\",\n",
    "    \"order_purchase_timestamp\", \"order_approved_at\",\n",
    "    \"order_delivered_carrier_date\", \"order_delivered_customer_date\",\n",
    "    \"order_estimated_delivery_date\",\n",
    "    \"purchase_date_id\", \"delivered_date_id\",\n",
    "    \"is_delivered\", \"is_canceled_or_unavailable\",\n",
    "    \"delivery_days\", \"on_time_flag\",\n",
    "    \"payment_total\", \"payment_count\"\n",
    "]]\n",
    "\n",
    "fact_orders.to_csv(OUT_DIR/\"fact_orders.csv\", index=False)\n",
    "\n",
    "# 7) Print checkpoints\n",
    "patched_count = mask_missing_pay.sum()\n",
    "still_missing = ((fact_orders[\"order_status\"] == \"delivered\") & (fact_orders[\"payment_total\"].isna())).sum()\n",
    "\n",
    "print(\"fact_orders saved:\", fact_orders.shape)\n",
    "print(\"patched delivered orders (missing payments):\", int(patched_count))\n",
    "print(\"delivered orders still missing payment_total:\", int(still_missing))\n",
    "print(\"null customer_unique_id:\", int(fact_orders[\"customer_unique_id\"].isna().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_order_items saved: (112650, 9)\n",
      "share_sum ~1 rate: 100.0 %\n",
      "null allocated_revenue: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fact_order_items = items.copy()\n",
    "\n",
    "# Derived item value\n",
    "fact_order_items[\"item_value\"] = fact_order_items[\"price\"] + fact_order_items[\"freight_value\"]\n",
    "\n",
    "# Join payment_total from fact_orders for allocation\n",
    "fact_order_items = fact_order_items.merge(\n",
    "    fact_orders[[\"order_id\", \"payment_total\"]],\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Compute item share within each order\n",
    "order_item_total = fact_order_items.groupby(\"order_id\")[\"item_value\"].transform(\"sum\").replace({0: np.nan})\n",
    "fact_order_items[\"order_item_value_share\"] = fact_order_items[\"item_value\"] / order_item_total\n",
    "\n",
    "# Allocate revenue back down to items\n",
    "fact_order_items[\"allocated_revenue\"] = fact_order_items[\"payment_total\"] * fact_order_items[\"order_item_value_share\"]\n",
    "\n",
    "# Keep curated columns\n",
    "fact_order_items = fact_order_items[[\n",
    "    \"order_id\", \"order_item_id\", \"product_id\",\n",
    "    \"shipping_limit_date\", \"price\", \"freight_value\",\n",
    "    \"item_value\", \"order_item_value_share\", \"allocated_revenue\"\n",
    "]]\n",
    "\n",
    "fact_order_items.to_csv(OUT_DIR/\"fact_order_items.csv\", index=False)\n",
    "\n",
    "# Checkpoint: do shares sum to ~1 per order?\n",
    "share_sum = fact_order_items.groupby(\"order_id\")[\"order_item_value_share\"].sum()\n",
    "ok_rate = share_sum.between(0.999, 1.001).mean()\n",
    "\n",
    "print(\"fact_order_items saved:\", fact_order_items.shape)\n",
    "print(\"share_sum ~1 rate:\", round(ok_rate*100, 2), \"%\")\n",
    "print(\"null allocated_revenue:\", int(fact_order_items[\"allocated_revenue\"].isna().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote DQ report: c:\\Users\\TAN\\OneDrive\\Cloud_KPI_Reporting_Pipeline\\reports\\data_quality\\data_quality_report_2026-01-12.md\n",
      "Overall: PASS\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "REPORT_DIR = ROOT / \"reports\" / \"data_quality\"\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "run_date = \"2026-01-12\"\n",
    "report_path = REPORT_DIR / f\"data_quality_report_{run_date}.md\"\n",
    "\n",
    "# Raw counts\n",
    "raw_counts = {\n",
    "    \"orders\": len(orders),\n",
    "    \"order_items\": len(items),\n",
    "    \"order_payments\": len(payments),\n",
    "    \"customers\": len(customers),\n",
    "    \"products\": len(products),\n",
    "    \"translation\": len(trans),\n",
    "}\n",
    "\n",
    "# Curated counts\n",
    "curated_counts = {\n",
    "    \"dim_date\": len(dim_date),\n",
    "    \"dim_customer\": len(dim_customer),\n",
    "    \"dim_product\": len(dim_product),\n",
    "    \"fact_orders\": len(fact_orders),\n",
    "    \"fact_order_items\": len(fact_order_items),\n",
    "}\n",
    "\n",
    "# BLOCKER checks\n",
    "blockers = []\n",
    "blockers.append((\"fact_orders order_id unique\", fact_orders[\"order_id\"].is_unique))\n",
    "blockers.append((\"fact_orders order_id not null\", fact_orders[\"order_id\"].notna().all()))\n",
    "blockers.append((\"fact_orders customer_unique_id not null\", fact_orders[\"customer_unique_id\"].notna().all()))\n",
    "blockers.append((\"fact_orders purchase_timestamp not null\", fact_orders[\"order_purchase_timestamp\"].notna().all()))\n",
    "blockers.append((\"fact_order_items (order_id, order_item_id) unique\",\n",
    "                 fact_order_items.set_index([\"order_id\",\"order_item_id\"]).index.is_unique))\n",
    "blockers.append((\"fact_order_items product_id not null\", fact_order_items[\"product_id\"].notna().all()))\n",
    "blockers.append((\"fact_order_items price >= 0\", (fact_order_items[\"price\"] >= 0).all()))\n",
    "blockers.append((\"fact_order_items freight_value >= 0\", (fact_order_items[\"freight_value\"] >= 0).all()))\n",
    "\n",
    "delivered_missing = ((fact_orders[\"order_status\"]==\"delivered\") & (fact_orders[\"payment_total\"].isna())).sum()\n",
    "blockers.append((\"delivered orders payment_total present\", delivered_missing == 0))\n",
    "\n",
    "cust_set = set(dim_customer[\"customer_unique_id\"])\n",
    "blockers.append((\"RI: fact_orders.customer_unique_id in dim_customer\",\n",
    "                 fact_orders[\"customer_unique_id\"].isin(cust_set).all()))\n",
    "\n",
    "prod_set = set(dim_product[\"product_id\"])\n",
    "blockers.append((\"RI: fact_order_items.product_id in dim_product\",\n",
    "                 fact_order_items[\"product_id\"].isin(prod_set).all()))\n",
    "\n",
    "# WARNING checks\n",
    "coverage_en = dim_product[\"product_category_name_en\"].notna().mean()\n",
    "share_sum = fact_order_items.groupby(\"order_id\")[\"order_item_value_share\"].sum()\n",
    "share_ok_rate = share_sum.between(0.999, 1.001).mean()\n",
    "\n",
    "overall_pass = all(ok for _, ok in blockers)\n",
    "\n",
    "patched_count = int(((orders[\"order_status\"]==\"delivered\")\n",
    "                     .values).sum())  # not used; keep report note below\n",
    "\n",
    "def pct(x): \n",
    "    return f\"{x*100:.2f}%\"\n",
    "\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"# Data Quality Report — {run_date}\\n\\n\")\n",
    "    f.write(\"## Run Metadata\\n\")\n",
    "    f.write(f\"- Snapshot: data/raw/olist/2026-01-12/\\n\")\n",
    "    f.write(f\"- Curated output: data/curated/olist/2026-01-12/\\n\")\n",
    "    f.write(f\"- Overall status: {'PASS' if overall_pass else 'FAIL'}\\n\\n\")\n",
    "\n",
    "    f.write(\"## Row Counts\\n\")\n",
    "    f.write(\"| Table | Rows |\\n|---|---:|\\n\")\n",
    "    for k,v in raw_counts.items():\n",
    "        f.write(f\"| raw.{k} | {v} |\\n\")\n",
    "    for k,v in curated_counts.items():\n",
    "        f.write(f\"| curated.{k} | {v} |\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"## BLOCKER Checks\\n\")\n",
    "    f.write(\"| Check | Result |\\n|---|---|\\n\")\n",
    "    for name, ok in blockers:\n",
    "        f.write(f\"| {name} | {'PASS' if ok else 'FAIL'} |\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"## WARNING Checks\\n\")\n",
    "    f.write(\"| Check | Value |\\n|---|---|\\n\")\n",
    "    f.write(f\"| Category EN coverage | {pct(coverage_en)} |\\n\")\n",
    "    f.write(f\"| Share sum ≈ 1 (rate) | {pct(share_ok_rate)} |\\n\\n\")\n",
    "\n",
    "    f.write(\"## Notes\\n\")\n",
    "    f.write(\"- 1 delivered order had missing payment rows; payment_total was patched using SUM(price + freight) for that order.\\n\")\n",
    "\n",
    "print(\"Wrote DQ report:\", report_path)\n",
    "print(\"Overall:\", \"PASS\" if overall_pass else \"FAIL\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
